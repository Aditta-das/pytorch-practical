{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "save model by pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuVX9CzP7kV4",
        "outputId": "e03c8198-7677-4441-fdb2-649a78c2cbf3"
      },
      "source": [
        "# Imports\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\r\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\r\n",
        "import torch.nn.functional as F  # All functions that don't have any parameters\r\n",
        "from torch.utils.data import (\r\n",
        "    DataLoader,\r\n",
        ")  # Gives easier dataset managment and creates mini batches\r\n",
        "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\r\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\r\n",
        "\r\n",
        "# Set device\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)\r\n",
        "# Hyperparameters\r\n",
        "input_size = 28\r\n",
        "hidden_size = 256\r\n",
        "num_layers = 2\r\n",
        "num_classes = 10\r\n",
        "num_epochs = 10\r\n",
        "sequence_length = 28\r\n",
        "learning_rate = 0.001\r\n",
        "batch_size = 64\r\n",
        "load_model = True\r\n",
        "# Recurrent neural network (many-to-one)\r\n",
        "class RNN(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\r\n",
        "        super(RNN, self).__init__()\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.num_layers = num_layers\r\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\r\n",
        "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\r\n",
        "        \r\n",
        "    # forward\r\n",
        "    def forward(self, x):\r\n",
        "        # set initial hidden and cell state\r\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\r\n",
        "        # forward propagate\r\n",
        "        out, _ = self.gru(x, h0)\r\n",
        "        out = out.reshape(out.shape[0], -1)\r\n",
        "        # decode the hidden state\r\n",
        "        out = self.fc(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "def save_checkpoint(state, filename=\"mycheckpoint.pth.tar\"):\r\n",
        "  print(\"Saving Checkpoint\")\r\n",
        "  torch.save(state, filename)\r\n",
        "\r\n",
        "def load_checkpoint(checkpoint):\r\n",
        "  print(\"Load Checkpoint\")\r\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\r\n",
        "  optimizer.load_state_dict(checkpoint[\"optimizer\"])\r\n",
        "\r\n",
        "# Load Data\r\n",
        "train_dataset = datasets.MNIST(\r\n",
        "    root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True\r\n",
        ")\r\n",
        "\r\n",
        "test_dataset = datasets.MNIST(\r\n",
        "    root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True\r\n",
        ")\r\n",
        "\r\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\r\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\r\n",
        "\r\n",
        "# Initialize network\r\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes)\r\n",
        "\r\n",
        "# Loss and optimizer\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n",
        "if load_model:\r\n",
        "  load_checkpoint(torch.load(\"/content/mycheckpoint.pth.tar\"))\r\n",
        "\r\n",
        "# Train Network\r\n",
        "for epoch in range(num_epochs):  \r\n",
        "  losses = []\r\n",
        "  if epoch % 3 == 0:\r\n",
        "    checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\r\n",
        "    save_checkpoint(checkpoint)\r\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\r\n",
        "    data = data.to(device=device).squeeze(1)\r\n",
        "    targets = targets.to(device=device)\r\n",
        "\r\n",
        "    # forward\r\n",
        "    scores = model(data)\r\n",
        "    loss = criterion(scores, targets)\r\n",
        "    losses.append(loss.item())\r\n",
        "    # backward \r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "  mean_loss = sum(losses) / len(losses)\r\n",
        "  print(f\"Loss at epoch {epoch} was {mean_loss : .5f}\")\r\n",
        "\r\n",
        "# check accuracy\r\n",
        "# def check_accuracy(loader, model):\r\n",
        "#     if loader.dataset.train:\r\n",
        "#         print(\"Checking accuracy on train\")\r\n",
        "#     else:\r\n",
        "#         print(\"checking accuracy on test\")\r\n",
        "        \r\n",
        "#     num_correct = 0\r\n",
        "#     num_samples = 0\r\n",
        "#     model.eval()\r\n",
        "#     with torch.no_grad():\r\n",
        "#         for x, y in loader:\r\n",
        "#             x = x.to(device=device).squeeze(1)\r\n",
        "#             y = y.to(device=device)\r\n",
        "#             scores = model(x)\r\n",
        "#             _, predictions = scores.max(1)\r\n",
        "#             num_correct += (predictions == y).sum()\r\n",
        "#             num_samples += predictions.size(0)\r\n",
        "#         print(\r\n",
        "#             f\"Got {num_correct} / {num_samples} with \\\r\n",
        "#               accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\r\n",
        "#         )\r\n",
        "#     model.train()\r\n",
        "\r\n",
        "# check_accuracy(train_loader, model)\r\n",
        "# check_accuracy(test_loader, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n",
            "Load Checkpoint\n",
            "Saving Checkpoint\n",
            "Loss at epoch 0 was  0.19813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY0tMAd_7rPQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}